<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="color-scheme" content="light dark" />
  <title>Workshop: New Perspectives on Bias and Discrimination in Language Technology.</title>
  <meta name="description" content="Workshop: New Perspectives on Bias and Discrimination in Language Technology." />

  <link rel="stylesheet" href="style.css">
  <!-- Pico.css -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@picocss/pico@2.0.6/css/pico.min.css" />
  <script defer src="https://discreet-raccoon.pikapod.net/script.js" data-website-id="7e936b29-ab4a-4bf1-b25d-79be853c6777"></script>
</head>

<body>
  <!-- Header -->
  <header class="container">
    <img src="assets/banner.jpg">
    <hgroup>
      <h1>New Perspectives on Bias and Discrimination in Language Technology</h1>
      <p>4th & 5th of November, 2024 @University of Amsterdam</p>
    </hgroup>
    <nav>
      <ul>
        <li><a href="index.html#workshop">Workshop</a></li>
        <li><a href="#schedule">Program</a></li>
        <li><a href="#keynotes">Invited speakers</a></li>
        <li><a href="call-for-abstracts.html"><button>Call for abstracts</button></a></li>
      </ul>
    </nav>
  </header>
  <!-- Main -->
  <main class="container">
    <!-- Preview -->
    <section id="workshop">
      <h2>Workshop</h2>
      <p>
        One of the central issues discussed in the context of the societal impact of language technology is that machine
        learning systems can contribute to discrimination, for instance by propagating human biases and stereotypes.
        Despite efforts to address these issues, we are far from solving them.
      </p>
      <p>
        The goal of this workshop is to bring together researchers from different fields to discuss the state of the art
        on bias measurement and mitigation in language technology and to explore new avenues of approach. For more
        information, read our <i>Call for Abstracts.</i>
      </p>
      <p><strong>The <a href="call-for-abstracts.html#call-for-abstracts">Call for Abstracts</a> is currently closed!</strong></p>
          <p><mark><abbr title="Registration"
            data-tooltip="This is also possible if you don't submit an abstract.">Please sign up if you would like to participate.</abbr></mark></p>
          <a href="https://docs.google.com/forms/d/e/1FAIpQLSdLmYgD3_LftxawK4UyiM7lCVmMBDfCcuY66abS7qHomFeOTw/viewform?usp=sf_link"><button>Register for the workshop</button></a>
    </section>
    <h4>Organizers</h4>
    <p>This workshop is organized by Katrin Schulz, Leendert van Maanen, Jelle Zuidema, Oskar van der Wal and Dominik
      Bachmann as part of the project <i>“The biased reality of online media - Using stereotypes to make media
        manipulation visible”</i>, which is financed by the Dutch Research Council (NWO). In this project we integrate
      knowledge from AI, psychology and linguistics to develop measures for social biases in language models and humans,
      and then to use these to study the influence of our media consumption on our beliefs.</p>
    <h4>Important Dates</h4>

    <div class="overflow-auto">
      <table>
        <tbody>
          <tr>
            <td width="40%">• <s>Deadline <a href="#call-for-abstracts">Call for Abstracts</a>:</s></td>
            <td><s><mark>15 September, 2024</mark></s></td>
          </tr>
          <tr>
            <td>• <s>Notification of acceptance:</s></td>
            <td><s><mark>24 September, 2024</mark></s></td>
          </tr>
          <tr>
            <td>• Workshop:</td>
            <td><mark>4 & 5 November, 2024</mark></td>
          </tr>
        </tbody>
      </table>
    </div>
    <section id="schedule">
      <h3>Program (tentative)</h3>
      <p>This workshop will be held in the <a href="https://www.google.nl/maps/place/Matrix+Innovation+Center/@52.356361,4.9562824,17z/data=!3m1!4b1!4m5!3m4!1s0x47c6094580ce7efb:0x6f59f5ccdc976da0!8m2!3d52.3563577!4d4.9584711?shorturl=1">Matrix One building at Amsterdam Science Park</a>. For questions or comments, please contact Oskar via <a href="mailto: o.d.vanderwal@uva.nl">
          o.d.vanderwal@uva.nl</a>.</p>
      <details>
        <summary><u>Monday, 4th of November</u> (click to expand)</summary>
        <!-- <p><kbd>TBD.</kbd></p> -->
        <table>
          <tr>
            <td width="10%">09:00</td> <td><i>doors open, coffee</i></td>
          </tr>
          <tr>
            <td>09:30</td> <td>welcome</td>
          </tr>
          <tr>
            <td>09:45</td> <td>keynote 1: Dong Nguyen</td>
          </tr>
          <tr>
            <td>11:00</td> <td>☕ <i><abbr title="Coffee/Lunch"
              data-tooltip="Will be provided by us.">coffee</abbr></i></td>
          </tr>
          <tr>
            <td>11:15</td> <td>Vera Neplenbroek, Arianna Bisazza and Raquel Fernández: MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs</td>
          </tr>
          <tr><td>11:45</td><td>Beatrice Savoldi, Jasmijn Bastings, Luisa Bentivogli and Eva Vanmassenhove: A Decade of Gender Bias in Machine Translation</td></tr>
          <tr><td>12:15</td><td><i><abbr title="Coffee/Lunch"
            data-tooltip="Will be provided by us.">lunch</abbr></i></td></tr>
          <tr><td>13:45</td><td>keynote 2: John Lalor</td></tr>
          <tr><td>15:00</td><td>☕ <i><abbr title="Coffee/Lunch"
            data-tooltip="Will be provided by us.">coffee</abbr></i></td></tr>
          <tr><td>15:30</td><td>Flor Miriam Plaza del Arco, Amanda Cercas Curry, Alba Curry, Gavin Abercrombie and Dirk Hovy: Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution</td></tr>
          <tr><td>16:00</td><td>Hellina Hailu Nigatu and Zeerak Talat: A Capabilities Approach to Studying Bias and Harm in Language Technologies</td></tr>
          <tr><td>16:30</td><td>poster session with drinks</td></tr>
          <tr><td>18:00</td><td><u>end of day 1</u></td></tr>
        </table>
      </details>
      <details>
        <summary><u>Tuesday, 5th of November</u> (click to expand)</summary>
        <!-- <p><kbd>TBD.</kbd></p> -->
        <table>
          <tr><td>09:00</td><td><i>doors open, coffee</i></td></tr>
          <tr><td>10:00</td><td>keynote 3: Zeerak Talat</td></tr>
          <tr><td>11:15</td><td>☕ <i><abbr title="Coffee/Lunch"
            data-tooltip="Will be provided by us.">coffee</abbr></i></td></tr>
          <tr><td>11:30</td><td>Paula Helm and Gabor Bella: Resisting language modeling bias through pluriversal language technology design</td></tr>
          <tr><td>12:00</td><td>Oskar van der Wal and Dominik Bachmann: Undesirable Biases in NLP: Adressing Challenges of Measurement</td></tr>
          <tr><td>12:30</td><td><i><abbr title="Coffee/Lunch"
            data-tooltip="Will be provided by us.">lunch</abbr></i></td></tr>
          <tr><td>14:00</td><td>Panel discussion with Sally Haslanger (online) and Marjolein Lanzing: a philosophical perspective on algorithmic discrimination</td></tr>
          <tr><td>16:00</td><td><u>end of workshop</u></td></tr>
          <tr><td>18:00</td><td><i>dinner at <a href="https://www.google.com/maps/dir//Batjanstraat+1a,+1094+RC+Amsterdam/@52.3632165,4.8782973,13z/data=!4m8!4m7!1m0!1m5!1m1!1s0x47c6096ea08edab1:0x7b2b90955c520471!2m2!1d4.9383382!2d52.3594865?entry=ttu">Elixir</a> for speakers/poster presenters to join (but self-paid)</i></td></tr>
        </table>
      </details>
    </section>
    <section id="keynotes">
      <h3>Invited Speakers</h3>
      <p>We are excited to confirm the following <abbr title="Speakers"
          data-tooltip="More information about their talks will follow later.">invited speakers</abbr> at the
        workshop.</p>
      <article>
        <div class="keynote">
          <img class="profile" src="assets/Dong-Nguyen.jpg" alt="Dong Nguyen" width="170">
          <p>
            <strong>Dr. Dong Nguyen</strong> is assistant professor Computer Science at <em>Utrecht University</em>. She works in
            the field of Natural Language Processing and is the head of <em>the NLP and Society Lab</em>. Her research focuses on
            computational text analysis for research questions from the social sciences. In recent years, she has also
            worked on bias measurement and transparent NLP. [<a href="https://www.dongnguyen.nl/">Link to their website</a>]
          </p>
        </div>
        &nbsp;
        <details>
          <summary><strong>Keynote 1: When LLMs meet language variation: Taking stock and looking forward</strong> (click to view description)</summary>
          <i>Language inherently exhibits variation; speakers have access to a vast array of linguistic forms (e.g., words, grammatical constructions) to express "the same thing". Consider, for example, the different ways of pronouncing a given word, or the many creative spellings in social media posts. Such variation is not arbitrary; there is often a rich relationship between language variation and variables like age, gender, and regional background. Unfortunately, in NLP, language variation is frequently overlooked or dismissed as "noise".  In my presentation, I will explore the role of language variation in the development of Large Language Models (LLMs) and the implications for bias detection and mitigation. I will conclude by highlighting several key challenges that I believe will be important to address moving forward.</i>
        </details>
        <footer>
          <small>Monday 4th of November at 9:45.</small>
        </footer>
      </article>
      <article>
        <div class="keynote">
          <img class="profile" src="assets/John-Lalor.png" alt="John Lalor" width="170">
          <p>
            <strong>Dr. John Lalor</strong> is assistant professor of IT, Analytics, and Operations at <em>the University of
            Notre Dame, Indiana</em>. His research focuses on developing methods for evaluating machine learning (and
            especially Natural Language Processing) models and for quantifying uncertainty. [<a href="https://johnlalor.net/">Link to their website</a>]
          </p>
        </div>
        &nbsp;
        <details>
          <summary><strong>Keynote 2: Should Fairness be a Metric or a Model?</strong> (click to view description)</summary>
          <i>Identifying bias in NLP systems is often isolated and metric-driven. The presence or absence of bias is determined based on a single demographic dimension in an isolated context. As this area of study evolves, new methods are needed to account for biases across multiple demographics and complex environments. In this talk, I will discuss two emerging areas for fairness in NLP: measuring fairness across multiple demographics and modeling fairness in complex pipelines. I will present recent work on our efforts to address these topics and make the case for human-centered evaluation of model fairness assessment.</i>
        </details>
        <footer>
          <small>Monday 4th of November at 13:45.</small>
        </footer>
      </article>
    <article>
      <div class="keynote">
        <img class="profile" src="assets/Zeerak-Talat.jpg" alt="Zeerak Talat" width="170"> 
        <p>
        <strong>Dr. Zeerak Talat</strong> is a research fellow at <em>Mohamed Bin Zayed University of AI</em>, and in November, they will be a Chancellor's Fellow in Responsible ML and AI at <em>the Centre for Technomoral ML and AI at the Edinburg Futures Institute</em> and <em>Institute for Language, Cognition, and Computation</em>. Their research seeks to examine how machine learning systems interact with our societies and the downstream effects of introducing machine learning to our society. [<a href="https://zeerak.org/">Link to their website</a>]
        </p>
      </div>
      &nbsp;
        <details>
          <summary><strong>Keynote 3: On the promise of equitable machine learning technologies</strong> (click to view description)</summary>
          <i>Research on the production of social biases and fair machine learning technologies imply that there is an attainable goal, namely equitable machine learning technologies. However, the realism of this implication has received little attention. In this talk, I will discuss 3 factors that impact the realism of this implication and goal. First, I’ll discuss how the assumptions underlying the technologies lend themselves to equitable technologies. Then I will turn to how the nature of different types of work relate to the goal of equitable technologies. Finally, I’ll begin to discuss the political and material conditions to which research artefacts are subject to. In doing so, I will argue that developing equitable machine learning technologies faces potential insurmountable challenges.</i>
        </details>
        <footer>
          <small>Tuesday 5th of November at 9:30.</small>
        </footer>
    </article>
    <article>
      <div class="keynote">
        <img class="profile" src="assets/Abigail-Jacobs.jpg" alt="Abigail Jacobs" width="170"> 
        <p>
        <strong>Dr. Abigail Jacobs</strong> is assistant professor of Information at <em>the University of Michigan in the School of Information</em> and assistant professor of Complex Systems in <em>the College of Literature, Science, and the Arts</em>. Her current research interests are around measurement; the hidden assumptions in machine learning, focusing on measurement and validity as a lens; structure, governance, and inequality in sociotechnical systems; and social networks. [<a href="https://azjacobs.com/">Link to their website</a>]
        </p>
      </div>
      &nbsp;
        <details>
          <summary><strong>Keynote 4: CANCELED</strong> (click to view description)</summary>
          <i>Unfortunately, the speaker had to cancel this talk.</i>
        </details>
        <footer>
            <small><s>Tuesday 5th of November at 14:30</s> Canceled.</small>
            </footer>
        </article>
    </section>
  </main>
  <!-- ./ Main -->

  <!-- Footer -->
  <footer class="container">
    <small>Website created by <a href="https://odvanderwal.nl">Oskar</a> with Pico CSS • <a
        href="https://certain-ai.nl/projects/improving-language-model-bias-measures/">more information about our
        research</a>
    </small>
  </footer>
  <!-- ./ Footer -->
</body>

</html>
